{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "490c7294-6634-4e02-9808-9cd5e7cfafbb",
   "metadata": {},
   "source": [
    "## Qwen 2.5 Function Calling 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e8e7b1-8720-40bf-b44d-71b87987094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-21 08:55:17 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import yfinance as yf\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d3067e-c601-4e6d-ad7a-072a389867be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-21 08:55:41 [config.py:600] This model supports multiple tasks: {'score', 'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 04-21 08:55:41 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 04-21 08:55:44 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 04-21 08:55:47 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x145fa22d9210>\n",
      "INFO 04-21 08:55:48 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-21 08:55:48 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-21 08:55:48 [gpu_model_runner.py:1258] Starting to load model Qwen/Qwen2.5-7B-Instruct...\n",
      "WARNING 04-21 08:55:49 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 04-21 08:55:49 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393d62ec82824820a15781b863d3b2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-21 08:57:07 [loader.py:447] Loading weights took 77.96 seconds\n",
      "INFO 04-21 08:57:08 [gpu_model_runner.py:1273] Model loading took 14.2487 GiB and 79.402409 seconds\n",
      "INFO 04-21 08:57:24 [backends.py:416] Using cache directory: /giant-data/user/1111332/.cache/vllm/torch_compile_cache/450ea10ccb/rank_0_0 for vLLM's torch.compile\n",
      "INFO 04-21 08:57:24 [backends.py:426] Dynamo bytecode transform time: 16.04 s\n",
      "INFO 04-21 08:57:25 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 04-21 08:57:38 [monitor.py:33] torch.compile takes 16.04 s in total\n",
      "INFO 04-21 08:57:39 [kv_cache_utils.py:578] GPU KV cache size: 951,888 tokens\n",
      "INFO 04-21 08:57:39 [kv_cache_utils.py:581] Maximum concurrency for 32,768 tokens per request: 29.05x\n",
      "INFO 04-21 08:58:14 [gpu_model_runner.py:1608] Graph capturing finished in 35 secs, took 1.45 GiB\n",
      "INFO 04-21 08:58:14 [core.py:162] init engine (profile, create kv cache, warmup model) took 66.15 seconds\n"
     ]
    }
   ],
   "source": [
    "# 모델 및 토크나이저 초기화\n",
    "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "model = LLM(MODEL_ID, tensor_parallel_size=1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "191512f7-cce6-4128-87ac-c02526a331ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플링 파라미터 설정 - Qwen 2.5에 맞게 stop token 변경\n",
    "sampling_params_func_call = SamplingParams(\n",
    "    max_tokens=256, temperature=0.0, stop=[\"<|im_end|>\"], skip_special_tokens=False\n",
    ")\n",
    "sampling_params_text = SamplingParams(\n",
    "    max_tokens=512, temperature=0.1, top_p=0.95, stop=[\"<|im_end|>\"], skip_special_tokens=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51095477-f75f-400a-ba10-8c42b1c949ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOSPI 주식 정보\n",
    "KOSPI_TICKER_MAP = {\n",
    "    \"SK텔레콤\": \"017670.KS\", \"삼성전자\": \"005930.KS\", \"SK하이닉스\": \"000660.KS\",\n",
    "    \"현대차\": \"005380.KS\", \"기아\": \"000270.KS\", \"LG에너지솔루션\": \"373220.KS\",\n",
    "    \"NAVER\": \"035420.KS\", \"카카오\": \"035720.KS\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "005499e5-0a83-462c-b8f8-07da475600b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구(함수) 정의\n",
    "TOOLS = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_kospi_stock_info\",\n",
    "        \"description\": \"특정 KOSPI 주식의 현재 가격 및 기본 정보를 가져옵니다.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"stock_name_or_code\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"주식 이름(예: 'SK텔레콤') 또는 종목 코드(예: '017670')\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"stock_name_or_code\"]\n",
    "        }\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4962e046-c8f1-4943-983a-6f268a0924a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주가 조회\n",
    "def get_kospi_stock_info(stock_name_or_code: str) -> str:\n",
    "\n",
    "    name_or_code = stock_name_or_code.strip()\n",
    "    ticker_symbol = None\n",
    "\n",
    "    if re.fullmatch(r'\\d{6}', name_or_code):\n",
    "        ticker_symbol = name_or_code + \".KS\"\n",
    "    elif name_or_code in KOSPI_TICKER_MAP:\n",
    "        ticker_symbol = KOSPI_TICKER_MAP[name_or_code]\n",
    "    else:\n",
    "        ticker_symbol = name_or_code + \".KS\"\n",
    "\n",
    "    stock = yf.Ticker(ticker_symbol)\n",
    "    stock_info = stock.info\n",
    "\n",
    "    current_price = stock_info.get('currentPrice')\n",
    "    previous_close = stock_info.get('previousClose')\n",
    "\n",
    "    price_to_use = current_price if current_price is not None else previous_close\n",
    "    price_display = round(price_to_use, 2) if price_to_use is not None else \"정보 없음\"\n",
    "    previous_close_display = round(previous_close, 2) if previous_close is not None else \"정보 없음\"\n",
    "\n",
    "    result = {\n",
    "        \"ticker\": ticker_symbol,\n",
    "        \"stock_name\": stock_info.get('shortName', name_or_code),\n",
    "        \"current_price\": price_display,\n",
    "        \"previous_close\": previous_close_display,\n",
    "        \"currency\": stock_info.get('currency', 'KRW')\n",
    "    }\n",
    "    \n",
    "    return json.dumps(result, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2b3f8c2-6a9f-4ce2-a12b-a85328d2c437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 함수 호출 파싱\n",
    "# Qwen LLM 출력에서 <tool_call>...</tool_call> 형식의 함수 호출을 파싱\n",
    "def parse_tool_calls(content: str):\n",
    "    \n",
    "    tool_calls = []\n",
    "    pattern = r\"<tool_call>(.*?)</tool_call>\"\n",
    "    matches = re.finditer(pattern, content, re.DOTALL)\n",
    "\n",
    "    last_match_end = 0\n",
    "    parsed_calls = []\n",
    "\n",
    "    for match in matches:\n",
    "        tool_call_content = match.group(1).strip()\n",
    "        func_data = json.loads(tool_call_content)\n",
    "\n",
    "        if isinstance(func_data.get(\"arguments\"), str):\n",
    "            func_data[\"arguments\"] = json.loads(func_data.get(\"arguments\"))\n",
    "\n",
    "        parsed_calls.append({\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": func_data.get(\"name\"),\n",
    "                \"arguments\": func_data.get(\"arguments\", {})\n",
    "            },\n",
    "            \"id\": f\"call_{match.start()}\"\n",
    "        })\n",
    "        last_match_end = match.end()\n",
    "\n",
    "    first_match_start = content.find(\"<tool_call>\")\n",
    "    prefix_text = content[:first_match_start].strip() if first_match_start != -1 else content.strip()\n",
    "    if not parsed_calls:\n",
    "        prefix_text = re.sub(r\"<\\|im_end\\|>\\s*$\", \"\", prefix_text).strip()\n",
    "\n",
    "    assistant_message = {\"role\": \"assistant\"}\n",
    "    if prefix_text:\n",
    "        assistant_message[\"content\"] = prefix_text\n",
    "    if parsed_calls:\n",
    "        assistant_message[\"tool_calls\"] = parsed_calls\n",
    "    if not prefix_text and not parsed_calls:\n",
    "         assistant_message[\"content\"] = \"\"\n",
    "\n",
    "    if \"content\" in assistant_message and assistant_message[\"content\"]:\n",
    "        assistant_message[\"content\"] = re.sub(r\"<\\|im_end\\|>\\s*$\", \"\", assistant_message[\"content\"]).strip()\n",
    "\n",
    "    return assistant_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d1e807-d648-49ea-b38a-1ba9699b62a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메인 쿼리 처리 함수 \n",
    "def query_kospi_info(query: str) -> str:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"You are a helpful assistant. Current Date: {current_date}\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    print(f\"\\n### 1단계 초기 메시지 작성:\\n{messages}\")\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tools=TOOLS, add_generation_prompt=True, tokenize=False\n",
    "    )\n",
    "    print(f\"\\n### 2단계 함수 선택/응답 생성을 위한 프롬프트 구성:\\n{prompt}\")\n",
    "\n",
    "    first_output = model.generate([prompt], sampling_params_func_call)[0].outputs[0].text\n",
    "    print(f\"\\n### 3단계 함수 호출/초기 응답을 위한 LLM 응답:\\n{first_output}\")\n",
    "\n",
    "    assistant_msg = parse_tool_calls(first_output)\n",
    "    messages.append(assistant_msg)\n",
    "    print(f\"\\n### 4단계 함수 호출 내용 파싱 및 메시지 추가:\\n{assistant_msg}\")\n",
    "\n",
    "    if assistant_msg.get(\"tool_calls\"):\n",
    "        for call in assistant_msg[\"tool_calls\"]:\n",
    "            fn = call[\"function\"][\"name\"]\n",
    "            args = call[\"function\"][\"arguments\"]\n",
    "            if fn == \"get_kospi_stock_info\":\n",
    "                result = get_kospi_stock_info(args[\"stock_name_or_code\"])\n",
    "            else:\n",
    "                result = json.dumps({\"error\": \"지원하지 않는 함수\"}, ensure_ascii=False)\n",
    "            print(f\"\\n### 5단계 함수 실행 결과 ({fn}):\\n{result}\")\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": call[\"id\"],\n",
    "                \"name\": fn,\n",
    "                \"content\": result\n",
    "            })\n",
    "\n",
    "        final_prompt = tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, tokenize=False\n",
    "        )\n",
    "        print(f\"\\n### 6단계 최종 응답 생성을 위한 프롬프트:\\n{final_prompt}\")\n",
    "\n",
    "        final_output = model.generate([final_prompt], sampling_params_text)[0].outputs[0].text\n",
    "        final_response = final_output.strip().rstrip(\"<|im_end|>\")\n",
    "        print(f\"\\n### 7단계 최종 LLM 응답 (정리 후):\\n{final_response}\")\n",
    "        return final_response\n",
    "    else:\n",
    "        print(\"LLM이 함수를 호출하지 않았습니다. 초기 응답을 반환합니다.\")\n",
    "        content = assistant_msg.get(\"content\", \"\").strip()\n",
    "        return content or \"응답 내용을 찾을 수 없습니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d063cadf-c056-4c8f-8a35-2114238f7db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " 질문: SK텔레콤의 주가를 알려줘\n",
      "==========================================\n",
      "\n",
      "### 1단계 초기 메시지 작성:\n",
      "[{'role': 'system', 'content': 'You are a helpful assistant. Current Date: 2025-04-21'}, {'role': 'user', 'content': 'SK텔레콤의 주가를 알려줘'}]\n",
      "\n",
      "### 2단계 함수 선택/응답 생성을 위한 프롬프트 구성:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant. Current Date: 2025-04-21\n",
      "\n",
      "# Tools\n",
      "\n",
      "You may call one or more functions to assist with the user query.\n",
      "\n",
      "You are provided with function signatures within <tools></tools> XML tags:\n",
      "<tools>\n",
      "{\"type\": \"function\", \"function\": {\"name\": \"get_kospi_stock_info\", \"description\": \"특정 KOSPI 주식의 현재 가격 및 기본 정보를 가져옵니다.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"stock_name_or_code\": {\"type\": \"string\", \"description\": \"주식 이름(예: 'SK텔레콤') 또는 종목 코드(예: '017670')\"}}, \"required\": [\"stock_name_or_code\"]}}}\n",
      "</tools>\n",
      "\n",
      "For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
      "<tool_call>\n",
      "{\"name\": <function-name>, \"arguments\": <args-json-object>}\n",
      "</tool_call><|im_end|>\n",
      "<|im_start|>user\n",
      "SK텔레콤의 주가를 알려줘<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s, est. speed input: 479.23 toks/s, output: 61.96 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 3단계 함수 호출/초기 응답을 위한 LLM 응답:\n",
      "<tool_call>\n",
      "{\"name\": \"get_kospi_stock_info\", \"arguments\": {\"stock_name_or_code\": \"SK텔레콤\"}}\n",
      "</tool_call>\n",
      "\n",
      "### 4단계 함수 호출 내용 파싱 및 메시지 추가:\n",
      "{'role': 'assistant', 'tool_calls': [{'type': 'function', 'function': {'name': 'get_kospi_stock_info', 'arguments': {'stock_name_or_code': 'SK텔레콤'}}, 'id': 'call_0'}]}\n",
      "\n",
      "### 5단계 함수 실행 결과 (get_kospi_stock_info):\n",
      "{\"ticker\": \"017670.KS\", \"stock_name\": \"SKTelecom\", \"current_price\": 57700.0, \"previous_close\": 57900.0, \"currency\": \"KRW\"}\n",
      "\n",
      "### 6단계 최종 응답 생성을 위한 프롬프트:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant. Current Date: 2025-04-21<|im_end|>\n",
      "<|im_start|>user\n",
      "SK텔레콤의 주가를 알려줘<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<tool_call>\n",
      "{\"name\": \"get_kospi_stock_info\", \"arguments\": {\"stock_name_or_code\": \"SK텔레콤\"}}\n",
      "</tool_call><|im_end|>\n",
      "<|im_start|>user\n",
      "<tool_response>\n",
      "{\"ticker\": \"017670.KS\", \"stock_name\": \"SKTelecom\", \"current_price\": 57700.0, \"previous_close\": 57900.0, \"currency\": \"KRW\"}\n",
      "</tool_response><|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, est. speed input: 225.75 toks/s, output: 73.17 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 7단계 최종 LLM 응답 (정리 후):\n",
      "SK텔레콤의 주가는 현재 57,700원입니다. 전일 대비로는 200원 하락했습니다. 화폐 단위는 한국 원(KRW)입니다.\n",
      "\n",
      " 답변: SK텔레콤의 주가는 현재 57,700원입니다. 전일 대비로는 200원 하락했습니다. 화폐 단위는 한국 원(KRW)입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    queries = [\n",
    "        \"SK텔레콤의 주가를 알려줘\",\n",
    "#        \"삼성전자 주가 얼마야?\"\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\n========================================\")\n",
    "        print(f\" 질문: {query}\")\n",
    "        print(f\"==========================================\")\n",
    "        response = query_kospi_info(query)\n",
    "        print(f\"\\n 답변: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ced63-6344-4c28-9ae8-0f1c348f0ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
