{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Huggingface login"
      ],
      "metadata": {
        "id": "xOuobr5pF3B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "vODCZe3VC2_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Axolotl 설치"
      ],
      "metadata": {
        "id": "mOkLl9Q0F78V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT9bN-kgAz7B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# Check so there is a gpu available, a T4(free tier) is enough to run this notebook\n",
        "assert (torch.cuda.is_available()==True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==\"2.1.2\"\n",
        "!pip install -e git+https://github.com/OpenAccess-AI-Collective/axolotl#egg=axolotl\n",
        "!pip install flash-attn==\"2.5.0\"\n",
        "!pip install deepspeed==\"0.13.1\"!pip install mlflow==\"2.13.0\""
      ],
      "metadata": {
        "id": "cW6JZJFKA2Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Dataset 추가"
      ],
      "metadata": {
        "id": "aXblNNmydI87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir data\n",
        "!wget -O ./data/ko_civil_service-multiturn.json https://github.com/superdom/blog/raw/main/062024-axolotl/ko_civil_service-multiturn.json\n",
        "%ls ./data | grep ko_civil_service-multiturn.json"
      ],
      "metadata": {
        "id": "ACLVL-KqGCYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training 환경 구성"
      ],
      "metadata": {
        "id": "5S6VNPg8dM1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "yaml_string = \"\"\"\n",
        "base_model: meta-llama/Meta-Llama-3-8B-Instruct\n",
        "model_type: LlamaForCausalLM\n",
        "tokenizer_type: AutoTokenizer\n",
        "\n",
        "load_in_8bit: false\n",
        "load_in_4bit: true\n",
        "strict: false\n",
        "\n",
        "chat_template: llama3\n",
        "datasets:\n",
        "  - path: data/ko_civil_service-multiturn.json\n",
        "    type: sharegpt\n",
        "    chat_template: llama3\n",
        "\n",
        "dataset_prepared_path:\n",
        "output_dir: ./outputs/llama-3-8b-Instruct-multiturn-bnb-4bit-qlora/multiturn\n",
        "\n",
        "sequence_len: 4096\n",
        "sample_packing: false\n",
        "pad_to_sequence_len: true\n",
        "\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "lora_r: 8\n",
        "lora_alpha: 16\n",
        "lora_dropout: 0.05\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "\n",
        "#wandb_project:\n",
        "#wandb_entity:\n",
        "#wandb_watch:\n",
        "#wandb_name:\n",
        "#wandb_log_model:\n",
        "\n",
        "gradient_accumulation_steps: 2\n",
        "micro_batch_size: 1\n",
        "num_epochs: 5\n",
        "optimizer: adamw_bnb_8bit\n",
        "lr_scheduler: cosine\n",
        "learning_rate: 0.0002\n",
        "\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: false\n",
        "fp16: true\n",
        "tf32: false\n",
        "\n",
        "gradient_checkpointing: true\n",
        "gradient_checkpointing_kwargs:\n",
        "  use_reentrant: true\n",
        "early_stopping_patience:\n",
        "resume_from_checkpoint:\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention: false\n",
        "flash_attention: false\n",
        "\n",
        "warmup_steps: 5\n",
        "evals_per_epoch: 1\n",
        "eval_table_size:\n",
        "saves_per_epoch: 1\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.0\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: <|end_of_text|>\n",
        "  eos_token: <|eot_id|>\n",
        "\"\"\"\n",
        "\n",
        "yaml_dict = yaml.safe_load(yaml_string)\n",
        "\n",
        "file_path = 'llama-3-8b-Instruct-bnb-4bit-qlora.yaml'\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    yaml.dump(yaml_dict, file)"
      ],
      "metadata": {
        "id": "l9GvtUgMA6ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training!"
      ],
      "metadata": {
        "id": "EMpNLyDvdSf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch -m axolotl.cli.train /content/llama-3-8b-Instruct-bnb-4bit-qlora.yaml"
      ],
      "metadata": {
        "id": "YUG9cWF1BNOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio로 Inference"
      ],
      "metadata": {
        "id": "ad9BwFWSdWPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch -m axolotl.cli.inference /content/llama-3-8b-Instruct-bnb-4bit-qlora.yaml \\\n",
        "    --qlora_model_dir=\"./outputs/llama-3-8b-Instruct-multiturn-bnb-4bit-qlora/multiturn\" --gradio"
      ],
      "metadata": {
        "id": "CUD18wlpBQ95"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}