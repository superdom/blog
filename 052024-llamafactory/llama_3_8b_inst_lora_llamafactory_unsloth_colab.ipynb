{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## LLaMA-Factory 코드 및 Dataset 준비"
      ],
      "metadata": {
        "id": "PtooigVSyc3h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRLdqJPHRqw8"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory/data\n",
        "!wget -O ko_civil_service_inst.json https://github.com/superdom/blog/raw/main/052024-llamafactory/ko_civil_service_inst.json\n",
        "%ls | grep ko_civil_service_int.json\n",
        "%cd ..\n",
        "%cd /content/LLaMA-Factory/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLaMA-Factory 및 Unsloth 설치"
      ],
      "metadata": {
        "id": "0KCK_7iMypPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers\n",
        "!pip install .[bitsandbytes]"
      ],
      "metadata": {
        "id": "gLfO2iBVTeBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Dataset 추가"
      ],
      "metadata": {
        "id": "p9EqQ4qVyupS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install jq -y"
      ],
      "metadata": {
        "id": "SNjEW7HJX0Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jq '. |= {\"ko_civil_service_inst\": {\"file_name\": \"ko_civil_service_inst.json\"}} + .' ./data/dataset_info.json > ./data/tmp.json && mv ./data/tmp.json ./data/dataset_info.json"
      ],
      "metadata": {
        "id": "uMz7G01oXFi-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head ./data/dataset_info.json"
      ],
      "metadata": {
        "id": "UV4IBi_yYfow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training 환경 구성"
      ],
      "metadata": {
        "id": "zNJqta_Dy2v_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"config\", exist_ok=True)"
      ],
      "metadata": {
        "id": "n_lKE-6AtdXc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_yaml = \"\"\"\n",
        "model_name_or_path: unsloth/llama-3-8b-Instruct-bnb-4bit\n",
        "quantization_bit: 4\n",
        "use_unsloth: true\n",
        "\n",
        "### method\n",
        "stage: sft\n",
        "do_train: true\n",
        "flash_attn: auto\n",
        "use_unsloth: true\n",
        "finetuning_type: lora\n",
        "lora_target: all\n",
        "lora_rank: 8\n",
        "lora_alpha: 16\n",
        "lora_dropout: 0\n",
        "\n",
        "### dataset\n",
        "dataset: ko_civil_service_inst\n",
        "template: gemma\n",
        "cutoff_len: 1024\n",
        "#max_samples: 1000\n",
        "#overwrite_cache: true\n",
        "preprocessing_num_workers: 16\n",
        "\n",
        "### output\n",
        "output_dir: output/llama-3-8b-Instruct-bnb-4bit/qlora\n",
        "logging_steps: 10\n",
        "save_steps: 500\n",
        "plot_loss: true\n",
        "overwrite_output_dir: true\n",
        "\n",
        "### train\n",
        "per_device_train_batch_size: 1\n",
        "gradient_accumulation_steps: 4\n",
        "learning_rate: 1.0e-4\n",
        "num_train_epochs: 3.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "fp16: true\n",
        "report_to: none\n",
        "\n",
        "### eval\n",
        "val_size: 0.1\n",
        "per_device_eval_batch_size: 1\n",
        "evaluation_strategy: steps\n",
        "eval_steps: 100\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4-zgwABDVXbx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"config/llama3-8b-instruct-bnb-4bit-unsloth.yaml\", \"w\") as file:\n",
        "    file.write(config_yaml)"
      ],
      "metadata": {
        "id": "UTStSbeLtr7Q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training!"
      ],
      "metadata": {
        "id": "CWDTDeHey9MQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!llamafactory-cli train config/llama3-8b-instruct-bnb-4bit-unsloth.yaml"
      ],
      "metadata": {
        "id": "gzrVFAwkZOHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat 모드로 Inference Test"
      ],
      "metadata": {
        "id": "6VYgkotWzDN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "  adapter_name_or_path=\"output/llama-3-8b-Instruct-bnb-4bit/qlora\",\n",
        "  template=\"llama3\",\n",
        "  finetuning_type=\"lora\",\n",
        "  quantization_bit=4,\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"History has been removed.\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ],
      "metadata": {
        "id": "fi5Bfx8lpcFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델을 병합하여 저장"
      ],
      "metadata": {
        "id": "YCBWfTuF1Mde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "FquKs6_xqICy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # Unsloth의 양자화 모델이 아닌 원래 llama 모델 지정\n",
        "  adapter_name_or_path=\"output/llama-3-8b-Instruct-bnb-4bit/qlora\",\n",
        "  template=\"llama3\",\n",
        "  finetuning_type=\"lora\",\n",
        "  export_dir=\"output/Meta-Llama-3-8B-Instruct\",\n",
        "  export_size=2,                                  # 모델을 몇GB로 나눠 분할 저장할지 지정\n",
        "  export_device=\"cpu\",                            # 모델 병합을 처리할 디바이스 지정 (cpu and cuda)\n",
        "  #export_hub_model_id=\"your_id/your_model\",\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)"
      ],
      "metadata": {
        "id": "ku9zJ7p2qL7m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!llamafactory-cli export merge_llama3.json"
      ],
      "metadata": {
        "id": "6fBrfvkt1Eda"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}